/**
 * Copyright 2013-2016 Axel Huebl, Heiko Burau, Rene Widera, Marco Garten
 *
 * This file is part of PIConGPU.
 *
 * PIConGPU is free software: you can redistribute it and/or modify
 * it under the terms of the GNU General Public License as published by
 * the Free Software Foundation, either version 3 of the License, or
 * (at your option) any later version.
 *
 * PIConGPU is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 * GNU General Public License for more details.
 *
 * You should have received a copy of the GNU General Public License
 * along with PIConGPU.
 * If not, see <http://www.gnu.org/licenses/>.
 */


#pragma once

#include "mappings/threads/ThreadCollective.hpp"
#include "mappings/elements/Vectorize.hpp"

namespace picongpu
{
namespace yeeSolver
{
using namespace PMacc;

template<class BlockDescription_, class CurlType_,
         typename T_ElemSize = typename PMacc::math::CT::make_Int<simDim,1>::type::vector_type>
struct kernelUpdateE
{
template<class EBox, class BBox, class Mapping, typename T_Acc>
DINLINE void operator()(const T_Acc& acc, EBox fieldE, BBox fieldB, Mapping mapper) const
{

    PMACC_AUTO(cachedB, CachedBox::create < 0, typename BBox::ValueType > (acc, BlockDescription_()));

    nvidia::functors::Assign assign;
    const DataSpace<simDim> block(mapper.getSuperCellIndex(DataSpace<simDim > (blockIdx)));
    const DataSpace<simDim> blockCell = block * MappingDesc::SuperCellSize::toRT();

    const DataSpace<simDim > threadIndex( DataSpace<simDim >(threadIdx) * T_ElemSize::toRT() );
    PMACC_AUTO(fieldBBlock, fieldB.shift(blockCell));

    constexpr int stride =
        PMacc::math::CT::volume<MappingDesc::SuperCellSize>::type::value /
        PMacc::math::CT::volume<T_ElemSize>::type::value;

    namespace mapElem = mappings::elements;


    /* \todo write me generic
     * kernel kann support (blockSize,elemSize) with (1,N) or (N,1)
     * therefore `threadIndex` which is strided can be used
     */
    ThreadCollective<BlockDescription_, stride> collective(threadIndex);
    collective(
        assign,
        cachedB,
        fieldBBlock
    );

    __syncthreads();

    const float_X c2 = SPEED_OF_LIGHT * SPEED_OF_LIGHT;
    const float_X dt = DELTA_T;

    CurlType_ curl;

    mapElem::vectorize<simDim>(
        [&]( const DataSpace<simDim>& idx )
        {
            fieldE(blockCell + threadIndex + idx ) += curl(cachedB.shift(threadIndex + idx)) * c2 * dt;
        },
        T_ElemSize::toRT(),
        mapElem::Contiguous()
    );
}
};


template<class BlockDescription_, class CurlType_,
         typename T_ElemSize = typename PMacc::math::CT::make_Int<simDim,1>::type::vector_type>
struct kernelUpdateBHalf
{
template< class EBox, class BBox, class Mapping, typename T_Acc>
DINLINE void operator()(const T_Acc& acc,
                                  BBox fieldB,
                                  EBox fieldE,
                                  Mapping mapper) const
{

    PMACC_AUTO(cachedE, CachedBox::create < 0, typename EBox::ValueType > (acc, BlockDescription_()));

    nvidia::functors::Assign assign;
    const DataSpace<simDim> block(mapper.getSuperCellIndex(DataSpace<simDim > (blockIdx)));
    const DataSpace<simDim> blockCell = block * MappingDesc::SuperCellSize::toRT();
    const DataSpace<simDim > threadIndex( DataSpace<simDim >(threadIdx) * T_ElemSize::toRT() );
    PMACC_AUTO(fieldEBlock, fieldE.shift(blockCell));

    constexpr int stride =
        PMacc::math::CT::volume<MappingDesc::SuperCellSize>::type::value /
        PMacc::math::CT::volume<T_ElemSize>::type::value;

    namespace mapElem = mappings::elements;

    /* \todo write me generic
     * kernel kann support (blockSize,elemSize) with (1,N) or (N,1)
     * therefore `threadIndex` which is strided can be used
     */
    ThreadCollective<BlockDescription_, stride> collective(threadIndex);
    collective(
              assign,
              cachedE,
              fieldEBlock
              );

    __syncthreads();

    const float_X dt = DELTA_T;
    CurlType_ curl;

    mapElem::vectorize<simDim>(
        [&]( const DataSpace<simDim>& idx )
        {
            fieldB(blockCell + threadIndex + idx) -= curl(cachedE.shift(threadIndex + idx)) * float_X(0.5) * dt;
        },
        T_ElemSize::toRT()
    );
}
};

} // yeeSolver

} // picongpu