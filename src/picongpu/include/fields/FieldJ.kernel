/**
 * Copyright 2013-2016 Axel Huebl, Heiko Burau, Rene Widera, Marco Garten,
 *                     Benjamin Worpitz
 *
 * This file is part of PIConGPU.
 *
 * PIConGPU is free software: you can redistribute it and/or modify
 * it under the terms of the GNU General Public License as published by
 * the Free Software Foundation, either version 3 of the License, or
 * (at your option) any later version.
 *
 * PIConGPU is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 * GNU General Public License for more details.
 *
 * You should have received a copy of the GNU General Public License
 * along with PIConGPU.
 * If not, see <http://www.gnu.org/licenses/>.
 */

#pragma once

#include "pmacc_types.hpp"
#include "particles/frame_types.hpp"

#include "simulation_defines.hpp"

#include "FieldJ.hpp"
#include "particles/memory/boxes/ParticlesBox.hpp"


#include "algorithms/Velocity.hpp"

#include "memory/boxes/CachedBox.hpp"
#include "dimensions/DataSpaceOperations.hpp"
#include "nvidia/functors/Add.hpp"
#include "mappings/threads/ThreadCollective.hpp"
#include "algorithms/Set.hpp"

#include "particles/frame_types.hpp"

#include "mappings/elements/Vectorize.hpp"
#include "memory/dataTypes/Array.hpp"

namespace picongpu
{

using namespace PMacc;

typedef FieldJ::DataBoxType J_DataBox;

template< int workerMultiplier, class BlockDescription_, uint32_t AREA,
          typename T_ElemSize = typename PMacc::math::CT::make_Int<simDim,1>::type::vector_type >
struct kernelComputeCurrent
{
template< class JBox, class ParBox, class Mapping, class FrameSolver, typename T_Acc>
DINLINE void operator()(const T_Acc& acc, JBox fieldJ,
                                     ParBox boxPar, FrameSolver frameSolver, Mapping mapper) const
{
    namespace mapElem = mappings::elements;

    typedef typename ParBox::FrameType FrameType;
    typedef typename ParBox::FramePtr FramePtr;
    typedef typename Mapping::SuperCellSize SuperCellSize;

    const uint32_t cellsPerSuperCell = PMacc::math::CT::volume<SuperCellSize>::type::value;

    const DataSpace<simDim> block(mapper.getSuperCellIndex(DataSpace<simDim > (blockIdx)));
    const DataSpace<simDim > stridedThreadIndex( DataSpace<simDim >(threadIdx) * T_ElemSize::toRT() );

    /* thread id, can be greater than cellsPerSuperCell*/
    const int stridedLinearThreadIdx = DataSpaceOperations<simDim>::template map<SuperCellSize > (stridedThreadIndex);

    const uint32_t virtualBlockId = stridedLinearThreadIdx / cellsPerSuperCell;

#if (PMACC_CUDA_ENABLED != 1)
    // on cpu workerMultiplier must be set to one
    PMACC_CASSERT( workerMultiplier == 1 );
#endif

    FramePtr frame;
    lcellId_t particlesInSuperCell = 0;

    /* This memory is used by all virtual blocks*/
    PMACC_AUTO(cachedJ, CachedBox::create < 0, typename JBox::ValueType > (acc, BlockDescription_()));

    constexpr int stride =
        cellsPerSuperCell * workerMultiplier /
        PMacc::math::CT::volume<T_ElemSize>::type::value;


    frame = boxPar.getLastFrame(block);
    if (frame.isValid() && virtualBlockId == 0)
        particlesInSuperCell = boxPar.getSuperCell(block).getSizeLastFrame();

    /* select N-th (N=virtualBlockId) frame from the end of the list*/
    for (int i = 1; (i <= virtualBlockId) && frame.isValid(); ++i)
    {
        particlesInSuperCell = PMacc::math::CT::volume<SuperCellSize>::type::value;
        frame = boxPar.getPreviousFrame(frame);
    }

    Set<typename JBox::ValueType > set(float3_X::create(0.0));

    /* \todo write me generic
     * kernel kann support (blockSize,elemSize) with (1,N) or (N,1)
     * therefore `stridedThreadIndex` which is strided can be used
     */
    ThreadCollective<BlockDescription_, stride> collectiveSet(stridedThreadIndex);
    collectiveSet(set, cachedJ);

    __syncthreads();

    while (frame.isValid())
    {
        mapElem::vectorize<simDim>(
            [&]( const DataSpace<simDim>& idx )
            {
                DataSpace<simDim> threadIndex( stridedThreadIndex + idx );
                const int linearThreadIdx = DataSpaceOperations<simDim>::template map<SuperCellSize > ( threadIndex );
                const int virtualLinearId = linearThreadIdx - (virtualBlockId * cellsPerSuperCell);
                /* this test is only importend for the last frame
                 * if frame is not the last one particlesInSuperCell==particles count in supercell
                 */
                if (virtualLinearId < particlesInSuperCell)
                {
                    frameSolver(acc,
                                *frame,
                                virtualLinearId,
                                cachedJ);
                }
            },
            T_ElemSize::toRT()
        );

        particlesInSuperCell = PMacc::math::CT::volume<SuperCellSize>::type::value;
        for (int i = 0; (i < workerMultiplier) && frame.isValid(); ++i)
        {
            frame = boxPar.getPreviousFrame(frame);
        }
    }

    /* we wait that all threads finish the loop*/
    __syncthreads();

    nvidia::functors::Add add;


    /* \todo write me generic
     * kernel kann support (blockSize,elemSize) with (1,N) or (N,1)
     * therefore `stridedThreadIndex` which is strided can be used
     */
    const DataSpace<simDim> blockCell = block * SuperCellSize::toRT();
    ThreadCollective<BlockDescription_, stride> collectiveAdd(stridedThreadIndex);
    PMACC_AUTO(fieldJBlock, fieldJ.shift(blockCell));
    collectiveAdd(add, fieldJBlock, cachedJ);

}
};

template<class ParticleAlgo, class Velocity, class TVec>
struct ComputeCurrentPerFrame
{

    HDINLINE ComputeCurrentPerFrame(const float_X deltaTime) :
    m_deltaTime(deltaTime)
    {
    }

    template<class FrameType, class BoxJ , typename T_Acc>
    DINLINE void operator()(const T_Acc& acc, FrameType& frame, const int localIdx, BoxJ & jBox)
    {

        PMACC_AUTO(particle, frame[localIdx]);
        const float_X weighting = particle[weighting_];
        const floatD_X pos = particle[position_];
        const int particleCellIdx = particle[localCellIdx_];
        const float_X charge = attribute::getCharge(weighting,particle);
        const DataSpace<simDim> localCell(DataSpaceOperations<simDim>::template map<TVec > (particleCellIdx));

        Velocity velocity;
        const float3_X vel = velocity(
                                      particle[momentum_],
                                      attribute::getMass(weighting,particle));
        PMACC_AUTO(fieldJShiftToParticle, jBox.shift(localCell));
        ParticleAlgo perParticle;
        perParticle(acc,
                    fieldJShiftToParticle,
                    pos,
                    vel,
                    charge,
                    m_deltaTime
                    );
    }

private:
    PMACC_ALIGN(m_deltaTime, const float_X);
};

template<typename T_ElemSize = typename PMacc::math::CT::make_Int<simDim,1>::type::vector_type>
struct kernelAddCurrentToEMF
{
template<class T_CurrentInterpolation, class T_Mapping, typename T_Acc>
DINLINE void operator()(const T_Acc& acc, typename FieldE::DataBoxType fieldE,
                                      typename FieldB::DataBoxType fieldB,
                                      J_DataBox fieldJ,
                                      T_CurrentInterpolation currentInterpolation,
                                      T_Mapping mapper) const
{
    namespace mapElem = mappings::elements;
    /* Caching of fieldJ */
    typedef SuperCellDescription<
                SuperCellSize,
                typename T_CurrentInterpolation::LowerMargin,
                typename T_CurrentInterpolation::UpperMargin
                > BlockArea;

    PMACC_AUTO(cachedJ, CachedBox::create < 0, typename J_DataBox::ValueType > (acc, BlockArea()));

    nvidia::functors::Assign assign;
    const DataSpace<simDim> block(mapper.getSuperCellIndex(DataSpace<simDim > (blockIdx)));
    const DataSpace<simDim> blockCell = block * MappingDesc::SuperCellSize::toRT();

    const DataSpace<simDim> stridedThreadIndex( DataSpace<simDim >(threadIdx) * T_ElemSize::toRT() );

    PMACC_AUTO(fieldJBlock, fieldJ.shift(blockCell));

    constexpr int stride =
        PMacc::math::CT::volume<MappingDesc::SuperCellSize>::type::value /
        PMacc::math::CT::volume<T_ElemSize>::type::value;

    /* \todo write me generic
     * kernel kann support (blockSize,elemSize) with (1,N) or (N,1)
     * therefore `stridedThreadIndex` which is strided can be used
     */
    ThreadCollective<BlockArea,stride> collective(stridedThreadIndex);
    collective(
              assign,
              cachedJ,
              fieldJBlock
              );

    __syncthreads();

    mapElem::vectorize<simDim>(
        [&]( const DataSpace<simDim>& idx )
        {
            const DataSpace<simDim> threadIndex( stridedThreadIndex + idx );
            const DataSpace<T_Mapping::Dim> cell(blockCell + threadIndex);

            // Amperes Law:
            //   Change of the dE = - j / EPS0 * dt
            //                        j = current density (= current per area)
            //                          = fieldJ
            currentInterpolation( acc, fieldE.shift(cell), fieldB.shift(cell), cachedJ.shift(threadIndex) );
        },
        T_ElemSize::toRT()
    );
}
};

template<typename T_ElemSize = typename PMacc::math::CT::make_Int<simDim,1>::type::vector_type>
struct kernelBashCurrent
{
template<class Mapping, typename T_Acc>
DINLINE void operator()(const T_Acc& acc, J_DataBox fieldJ,
                                  J_DataBox targetJ,
                                  DataSpace<simDim> exchangeSize,
                                  DataSpace<simDim> direction,
                                  Mapping mapper) const
{
    namespace mapElem = mappings::elements;
    const DataSpace<simDim> blockCell(
                                      mapper.getSuperCellIndex(DataSpace<simDim > (blockIdx))
                                      * Mapping::SuperCellSize::toRT()
                                      );
    const DataSpace<simDim> stridedThreadIndex( DataSpace<simDim >(threadIdx) * T_ElemSize::toRT() );

    mapElem::vectorize<simDim>(
        [&]( const DataSpace<simDim>& idx )
        {
            DataSpace<simDim> threadIndex( stridedThreadIndex + idx );
            const DataSpace<Mapping::Dim> sourceCell(blockCell + threadIndex);

            /*origin in area from local GPU*/
            DataSpace<simDim> nullSourceCell(
                                             mapper.getSuperCellIndex(DataSpace<simDim > ())
                                             * Mapping::SuperCellSize::toRT()
                                             );
            DataSpace<simDim> targetCell(sourceCell - nullSourceCell);

            for (uint32_t d = 0; d < simDim; ++d)
            {
                if (direction[d] == -1)
                {
                    if (threadIndex[d] < SuperCellSize::toRT()[d] - exchangeSize[d]) return;
                    targetCell[d] -= SuperCellSize::toRT()[d] - exchangeSize[d];
                }
                else if ((direction[d] == 1) && (threadIndex[d] >= exchangeSize[d])) return;
            }

            targetJ(targetCell) = fieldJ(sourceCell);
        },
        T_ElemSize::toRT()
    );
}
};

template<typename T_ElemSize = typename PMacc::math::CT::make_Int<simDim,1>::type::vector_type >
struct kernelInsertCurrent
{
template<class Mapping, typename T_Acc>
DINLINE void operator()(const T_Acc& acc, J_DataBox fieldJ,
                                    J_DataBox sourceJ,
                                    DataSpace<simDim> exchangeSize,
                                    DataSpace<simDim> direction,
                                    Mapping mapper) const
{
    namespace mapElem = mappings::elements;

    const DataSpace<simDim> blockCell(
                                      mapper.getSuperCellIndex(DataSpace<simDim > (blockIdx))
                                      * Mapping::SuperCellSize::toRT()
                                      );

    const DataSpace<simDim> stridedThreadIndex( DataSpace<simDim >(threadIdx) * T_ElemSize::toRT() );

    mapElem::vectorize<simDim>(
        [&]( const DataSpace<simDim>& idx )
        {
            DataSpace<simDim> threadIndex( stridedThreadIndex + idx );
            DataSpace<Mapping::Dim> targetCell(blockCell + threadIndex);

            /*origin in area from local GPU*/
            DataSpace<simDim> nullSourceCell(
                                             mapper.getSuperCellIndex(DataSpace<simDim > ())
                                             * Mapping::SuperCellSize::toRT()
                                             );
            DataSpace<simDim> sourceCell(targetCell - nullSourceCell);

            for (uint32_t d = 0; d < simDim; ++d)
            {
                if (direction[d] == 1)
                {
                    if (threadIndex[d] < SuperCellSize::toRT()[d] - exchangeSize[d]) return;
                    sourceCell[d] -= SuperCellSize::toRT()[d] - exchangeSize[d];
                    targetCell[d] -= SuperCellSize::toRT()[d];
                }
                else if (direction[d] == -1)
                {
                    if (threadIndex[d] >= exchangeSize[d]) return;
                    targetCell[d] += SuperCellSize::toRT()[d];
                }
            }

            fieldJ(targetCell) += sourceJ(sourceCell);
        },
        T_ElemSize::toRT()
    );
}
};

}
